INFO 08-12 13:59:21 [__init__.py:244] Automatically detected platform cuda.
running with arguments:
  benchmark: BenchmarkDataset.GSM8K
  model_name: /home/ubuntu/its_hub/checkpoints/gsm8k_qwen_grpo_empo_nokl
  alg: ScalingAlgorithm.PARTICLE_FILTERING
  rm_device: cuda:0
  endpoint: http://0.0.0.0:8002/v1
  shuffle_seed: 1110
  does_eval: True
  budgets: [8]
  rm_agg_method: AggregationMethod.MODEL
  is_async: False
  max_tokens: None
  temperature: None
  max_concurrency: 8
  api_key: NO_API_KEY
  rm_name: Qwen/Qwen2.5-Math-PRM-7B
  subset: None
  output_dir: results
  force_run: False
  eval_expected_pass_at_one: False
  display_only: False
loading existing results...
loading benchmark dataset...
creating language model...
initializing algorithm...
Number of GPUs: 1
WARNING 08-12 13:59:44 [arg_utils.py:1642] --task reward is not supported by the V1 Engine. Falling back to V0. 
INFO 08-12 13:59:44 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='Qwen/Qwen2.5-Math-PRM-7B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-PRM-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-Math-PRM-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 08-12 13:59:46 [cuda.py:327] Using Flash Attention backend.
INFO 08-12 13:59:47 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-12 13:59:47 [model_runner.py:1171] Starting to load model Qwen/Qwen2.5-Math-PRM-7B...
INFO 08-12 13:59:47 [weight_utils.py:292] Using model weights format ['*.safetensors']
INFO 08-12 13:59:50 [default_loader.py:272] Loading weights took 2.81 seconds
INFO 08-12 13:59:51 [model_runner.py:1203] Model loading took 13.2502 GiB and 3.226475 seconds
running inference-time scaling for budgets=[8]...
error scaling example 1158: The decoder prompt (length 4099) is longer than the maximum model length of 4096. Make sure that `max_model_len` is no smaller than the number of text tokens.
error scaling example 320: The decoder prompt (length 4099) is longer than the maximum model length of 4096. Make sure that `max_model_len` is no smaller than the number of text tokens.
error scaling example 359: The decoder prompt (length 4098) is longer than the maximum model length of 4096. Make sure that `max_model_len` is no smaller than the number of text tokens.
error scaling example 814: The decoder prompt (length 4099) is longer than the maximum model length of 4096. Make sure that `max_model_len` is no smaller than the number of text tokens.
error scaling example 894: The decoder prompt (length 4099) is longer than the maximum model length of 4096. Make sure that `max_model_len` is no smaller than the number of text tokens.
saving results to results/-home-ubuntu-its_hub-checkpoints-gsm8k_qwen_grpo_empo_nokl-particle-filtering-Qwen-Qwen2.5-Math-PRM-7B-model-gsm8k-N-8.jsonl...
budget=  8: accuracy=0.8610 (1131/1314)
budget=  8: accuracy_argmax=0.8607 (1131/1314)
